<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>about</title>
  </head>
  <body>
    <div id="about-page" class="page">
      <div class="row">
        <h1 class="step">SOUND STORIES</h1>
        <div class="column right">
          <h3>By Katherine Chui</h3>
          <h4>Data Visualization M.S 2025</h4>
          <h4>Parsons School of Design</h4>
        </div>
      </div>
      <div id="header">
        <p>
          Film is a multisensory and interdisciplinary art form that can be
          visualized into a more accessible and immersive experience. I hope
          this project can be a space for exploring and enjoying the visual
          beauty of sound, and the intersection of music, narrative, and design.
        </p>
      </div>
      <section id="sketch-section">
        <div id="pause-play-container"></div>
        <div id="howcontainer"></div>
      </section>
      <section class="scroll-container" id="letter-section">
        <div class="sticky-container">
          <div class="caption" id="letter-caption">
            <h3>creating a</h3>
            <h2>dynamic font</h2>
          </div>
          <div id="letter-lottie"></div>
        </div>
        <div class="scrolly">
          <div class="stepl"></div>
        </div>
      </section>

      <div class="writeup">
        <h2 class="step"><span>Introduction</span></h2>
        <h3 class="c16" id="h.zavnm5lu4544">
          <span class="c24">Sound and Music in Film</span>
        </h3>
        <p class="c12">
          <span class="c0"
            >Sound is an important aspect of modern day film and storytelling.
            It comes in three main forms according to Jessica Green: dialogue
            (sounds made by characters in the film), noise, and music</span
          ><span>&nbsp;</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup
          ><span class="c1 c0"
            >. All of these aspects help create the atmosphere of a movie:
            setting the tone and signifying important plot cues to the
            audience.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >In films, music does everything from setting tones and conveying
            intensity to foreshadowing and reminding its audience of past
            scenes. Stam, Burgoyne, and Flitterman-Lewis break music into
            categories including: </span
          ><span>r</span
          ><span class="c0"
            >edundant music which matches the main emotion of a scene,
            contrapuntal music which runs counter to that emotion, </span
          ><span>e</span
          ><span class="c0"
            >mpathetic music which conveys the characters emotions, didactic
            contrapuntal music which works to distance the audience from a
            scene, and &nbsp;a-empathetic music which is neutral.</span
          ><span>&nbsp;</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >Music can work to develop motifs, signify themes, and build empathy
            between characters and the audience. Consider the Pixar Film, </span
          ><span class="c0 c7">UP</span
          ><span class="c0"
            >; in the beginning they introduce a musical motif that represents
            the connection between the main character and his wife. This musical
            theme starts cheerful as they meet, fall in love, and get married.
            As they grow old and his wife passes, the theme morphs&ndash;its
            tempo slows, the pitches shift, and notes are played with a softer,
            more reflective tone. This scene feels emotional although the
            character on screen isn&rsquo;t speaking or crying. As he
            reorganizes his house, keeping his wifes chair next to his, the
            musical theme chimes in, reminding us of how they fell in love as
            kids and all the life they lived together. It makes us understand
            what the main character is feeling without words: sad, reflective,
            nostalgic, and desperate to hold onto every memory of her. </span
          ><span class="c0 c7">UP</span
          ><span class="c0"
            >&nbsp;develops this musical theme at the start and uses it
            throughout the film to remind us of the main character&rsquo;s
            motivations and memories. This</span
          ><span>&nbsp;is</span><span class="c0">&nbsp;just one example</span
          ><span>&nbsp;that </span
          ><span class="c1 c0"
            >shows how music can be incredibly powerful in storytelling.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16" id="h.5wwmv3wwgcbm">
          <span class="c1 c11">Objectives</span>
        </h3>
        <p class="c12">
          <span class="c1 c0"
            >In this project, my goal is to explore how sound and music can be
            conveyed in a movie using only visuals. To do this, I will focus
            primarily on the music and capturing the emotions, intensity, and
            patterns it conveys. I also hope to explore how captions can further
            support the music and dialogue of a story, which I&rsquo;ll do by
            building a dynamic typographic library to display captions that
            match the intensity of the movie sounds.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c1 c0"
            >In order to understand music in film from both a statistical and
            artistic point of view, this project is broken down into two levels
            of scale. The first level focuses on the imagery and captions that
            can be displayed in synchronization with a video and its audio; this
            level breaks a single video file into small time increments. The
            second level works to give a more general view of many videos in a
            movie showcasing different auditory trends across a full film.
            Through delving into both levels of generality, we can both
            experience music and the art that it creates as an audience member
            while also understanding sound in its larger context as it works to
            progress a story.</span
          >
        </p>
        <hr />
        <p class="c6"><span class="c1 c19"></span></p>
        <h2 class="step" id="h.z2l093htjvn3">
          <span class="c1 c13">Background</span>
        </h2>
        <h3 class="c16" id="h.dee211y43txp">
          <span class="c24">History of Music in Film</span>
        </h3>
        <p class="c12">
          <span class="c0"
            >In our modern era of entertainment, movies from Star Wars, Toy
            Story, Spirited Away, and Interstellar use sound effects, music, and
            dialogue to emphasize emotion, build intensity and action, and
            create music scores that people associate with each film after
            watching. This wasn&rsquo;t always the case; films in the 1890s to
            the 1920s were silent, either using live music in theaters like
            plays or relying on the expressions of actors to convey the
            film&#39;s narrative. &nbsp;</span
          ><span class="c0 c7">The Jazz Singer</span
          ><span class="c0">&nbsp;in 1927 </span
          ><span class="c0">by the Warner</span
          ><span class="c0"
            >&nbsp;Bros was the first movie with synchronized sound. In 1931,
            the </span
          ><span class="c0"
            >sound-on-film system inscribed sound waves as lightwaves onto the
            photographs, allowing soundtracks and videos to be stored in the
            same strip.</span
          ><sup class="c0"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup
          ><span class="c0"
            >&nbsp; During the war, comedies, action movies, and musicals
            increased in popularity. This increased the production of orchestra
            music composed to match the pacing and emotions of the films. As the
            media industry grew through the late 1900s, music in films started
            to merge with jazz, rock and roll, and pop artists. Furthermore,
            synthetic music techniques started being added to film in the 1980s
            with movies like </span
          ><span class="c0 c7">Tron</span><span class="c0">&nbsp;and </span
          ><span class="c0 c7">Star Wars. </span
          ><sup class="c0"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16" id="h.i050x0wzgdsj">
          <span class="c1 c11">Captions and their Significance</span>
        </h3>
        <p class="c12">
          <span class="c0">According to over 100 studies &nbsp;&ldquo;..</span
          ><span>. </span
          ><span class="c0"
            >captioning a video improves comprehension of, attention to, and
            memory for the video&rdquo; for all audiences. This includes people
            of all ages, those who are deaf or hard of hearing, and those
            watching movies in their non-native language. Historically, captions
            fi</span
          ><span
            >rst were introduced as &ldquo;intertitles&rdquo; which showed
            written dialogue in between scenes in silent films. </span
          ><span class="c0"
            >In the 1920, films with synchronized sound took over silent films,
            eliminating accessibility for the D/deaf community</span
          ><span>, and in </span
          ><span class="c0"
            >the 1950s, the federal government authorized captioned films for
            cold war education. Captioning didn&rsquo;t begin appearing on
            television until the 1970s-80s, and by the 1990s, captions on TV
            shows became mandatory by law in the US. </span
          ><sup class="c0"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup
          ><span class="c0">&nbsp;</span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16" id="h.ud9ip6dalwby">
          <span class="c1 c11">Limitations of Captions</span>
        </h3>
        <p class="c12">
          <span
            >Captions lead to greater comprehension and can enhance film
            experiences for all people. However, one existing limitation of
            movie captions is the written cues of a film&rsquo;s music. For
            example, many films denote sound changes through bracketed
            descriptions like: &ldquo;</span
          ><span class="c7">[Music intensifies]</span
          ><span>&rdquo; or &ldquo;</span
          ><span class="c7">[Emotional music plays]</span
          ><span
            >&rdquo;. I hope to address this limitation by creating visuals
            through typography, shapes, and colors to better translate the music
            of films into the visual world. In this project, I explore a range
            of audio data driven visual representations of music in film working
            to reflect the emotional tone, intensity, and thematic motifs that
            the original scores communicate.
          </span>
        </p>
        <p class="c6"><span class="c1 c19"></span></p>
        <hr />
        <p class="c6"><span class="c1 c19"></span></p>
        <h2 class="step" id="h.okveijyl2hzd">
          <span class="c20">Methods: </span
          ><span class="c1 c13">Data Collection and Processing </span>
        </h2>

        <p class="c12">
          <span class="c0"
            >The majority data in this project is from existing films and video
            clips which I then process into various datasets summarizing the
            audio and image elements of the videos. In the audio processing
            phase of this project, I used machine learning models which are
            trained on external datasets </span
          ><span>and</span
          ><span class="c1 c0"
            >&nbsp;are described in greater detail below.</span
          >
        </p>
        <video controls autoplay width="100%" height="auto" preload="metadata">
          <source src="./styles/animations/processing.mp4" type="video/mp4" />
        </video>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0">By analyzing the music on both a specific </span
          ><span>and</span
          ><span class="c0"
            >&nbsp;general level, it provides a picture of the experience of the
            music while also summarizing it as an overall progression of the
            movie. On the most zoomed in scale, I extract various audio features
            from sound clips which are </span
          ><span>stored</span><span class="c0">&nbsp;as</span
          ><span>&nbsp;continuous waveforms in .wav files</span
          ><span class="c1 c0"
            >. This includes features like amplitude, energy, tempo, the notes
            being played, the potential emotions represented in the sound, and
            the distribution of instruments being played.
          </span>
        </p>
        <h3 class="c16 c30" id="h.ayl1arflwbtv">
          <span class="c1 c11"></span>
        </h3>
        <h3 class="c16" id="h.j1mxqi1ryvcl">
          <span class="c1 c11">Video Processing</span>
        </h3>
        <p class="c12">
          <span class="c0"
            >Taking as input either a youtube link or a video file, I created a
            function which does the following actions. If the input is a youtube
            link, it downloads the video. This step uses </span
          ><span class="c0 c7">yt-dlp</span
          ><span class="c0">&nbsp;and saves it in a local folder</span
          ><span>.</span><span class="c0">&nbsp;</span><span>I</span
          ><span class="c0">t then breaks the inputted video into </span
          ><span class="c23">n</span
          ><span class="c0">&nbsp;sections by using </span
          ><span class="c0 c7">ffmpeg</span
          ><span class="c0">&nbsp;to split the video into </span
          ><span>clips of</span
          ><span class="c0">&nbsp;equal duration which it then saves </span
          ><span>locally</span
          ><span class="c0">. Using ffmpeg, it then extracts audio </span
          ><span>and </span><span class="c0">images from each of the </span
          ><span class="c23">n</span><span class="c0">&nbsp;video clips</span
          ><span>, saving them i</span
          ><span class="c1 c0"
            >nto &ldquo;audios&rdquo; and &ldquo;images&rdquo; folders
            respectively.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >These folders ( &ldquo;videos&rdquo;, &ldquo;audios&rdquo;,
            &ldquo;images&rdquo;) contain the raw data for our movie. This
            function then delegates to two other functions, </span
          ><span class="c0 c7">getAudioData</span
          ><span class="c0">&nbsp;and </span
          ><span class="c0 c7">getImageData</span
          ><span class="c0"
            >, to process the raw audio files and raw image files into .json </span
          ><span>data files</span><span class="c1 c0">. </span>
        </p>
        <p class="c12">
          <span class="c10 c3 c31"
            >The audio processing is the bulk of the data collection in this
            project, but the image processing provides necessary color
            information to create visualizations that are cohesive with each
            scene. </span
          ><span><img alt="" src="./styles/images/image3.png" title="" /></span>
        </p>
        <h3 class="c16" id="h.k2fsnfwbd42">
          <span class="c1 c11">Image Processing</span>
        </h3>
        <p class="c12">
          <span class="c0">From each image, I extracted the</span
          ><span>&nbsp;main color palette</span
          ><span class="c0"
            >. To start, I read in the pixel data and converted it into a
            dataframe where each row represented the color of a pixel broken
            down into its </span
          ><span>red, green, and blue</span><span class="c7">&nbsp;</span
          ><span class="c0">values </span><span class="c7">&nbsp;(R,G,B) </span
          ><span class="c1 c0"
            >. I then explored two methods for extracting the main colors.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c1 c0"
            >For the first method, I used a Gaussian Clustering model to cluster
            the pixels into 8 clusters. I then used the cluster center
            coordinates to get the average color for each cluster. Since our
            data has three dimensions (R,G,B), we can easily get the color of
            each cluster by taking the first coordinate of the cluster center to
            be the red component, the second coordinate to be the green
            component, and so on. I then output the top 5 most saturated colors
            out of the 8 clusters.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >The second method is conceptually very similar to a clustering
            algorithm, but works primarily to separate colors by a</span
          ><span class="c0 c7">&nbsp;color_distance_threshold</span
          ><span class="c0"
            >&nbsp;to create a list of unique colors. To do this, I iterate
            through each pixel in the image and add that color to a list of
            unique colors if it is &ldquo;unique&rdquo;. To determine if
            it&rsquo;s &ldquo;unique&rdquo;, I compute the euclidean distance
            (distance of the line between two points in </span
          ><span>&#8477;</span><span class="c32">3</span
          ><span class="c0"
            >) between that color and every color that is already in the list of
            unique colors; if the pairwise distances are all greater than the </span
          ><span class="c0 c7">color_distance_threshold</span
          ><span class="c0"
            >, then it considered &ldquo;unique&rdquo; and it is added to the
            unique colors list. This process ensures that all colors in the
            final unique color list have pairwise distances greater than </span
          ><span class="c0 c7">color_distance_threshold</span
          ><span class="c0"
            >. I then return the top 5 most saturated colors from this list. In
            practice, I chose a </span
          ><span class="c0 c7">color_distance_threshold</span
          ><span class="c1 c0">&nbsp;of 100.</span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image12.png" title="" /></span>
        </p>
        <h5 class="c22" id="h.n74ssw7k5866">
          <span
            >Color extraction of a scene from the film, &ldquo;Past Lives&rdquo;
            2023. This shows both methods with the selected colors as well as
            the pixels in </span
          ><img
            class="inline"
            src="./styles/images/image1.png"
            style="width: 10px"
          /><span class="c2"
            >space. We see that the Color distance threshold yields a brighter
            range of colors.
          </span>
        </h5>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >After previewing the colors extracted using both methods, I found
            that the second method produced a greater variety of colors which
            better represented each image. This is likely due to the fact that
            the </span
          ><span>clustering models</span
          ><span class="c0"
            >&nbsp;also factor in the size of each cluster causing these models
            to create more </span
          ><span>balanced</span
          ><span class="c0"
            >&nbsp;clusters. As a result, if an image has a lot of dark colored
            pixels, it&rsquo;ll end up returning mostly dark colors instead of
            picking out the few &ldquo;unique&rdquo; colored pixels.
            Additionally, the cluster colors </span
          ><span class="c1 c0"
            >are an average of all colors in the cluster, not necessarily the
            brightest color of each cluster, thus leading to more muted
            colors.</span
          ><span><img alt="" src="./styles/images/image7.png" title="" /></span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h5 class="c5 c25" id="h.zz3o51e2u2u">
          <span
            >Example or colors selected from a series of scenes from Studio
            Ghibli&rsquo;s &ldquo;Princess Mononoke&rdquo;</span
          >
        </h5>
        <h3 class="c16 c30" id="h.hyt0dd58v8wm">
          <span class="c1 c11"></span>
        </h3>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16 c30" id="h.sq0dpddrjz2p">
          <span class="c1 c11"></span>
        </h3>
        <h3 class="c16" id="h.gxg8tk6daqwo">
          <span class="c1 c11">Audio Processing</span>
        </h3>
        <p class="c12">
          <span class="c0"
            >The first step in this is extracting useful features from the audio
            file. For images, our goal was to understand the colors, so breaking
            down each pixel into its </span
          ><span>RGB</span><span class="c0">&nbsp;values </span
          ><span>was the obvious direction</span
          ><span class="c0"
            >. With the audio files, it&rsquo;s less straightforward which
            features will tell us what. For example, one feature of audio is its
            volume, but we could have two very different audio clips that both
            have similar volumes. Since we are exploring different
            visualizations, emotional tones, and intensities of music, we need
            to capture a variety of features from our audio.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span>I used the audio library, </span><span class="c7">librosa,</span
          ><span class="c1 c0"
            >&nbsp;to process a waveform at a given sample rate into numerical
            data such as spectrograms, tempograms, chromatograms, and mfcc
            constants.</span
          >
        </p>
        <h4 class="c12 c25 c35" id="h.mnpxqpkc9zhb">
          <span class="c1 c19"></span>
        </h4>
        <h4 class="c8" id="h.ou4mjolgtyjq">
          <span class="c1 c19">Initial Feature Extraction</span>
        </h4>
        <p class="c12">
          <span class="c0">My first goal </span><span>was</span
          ><span class="c1 c0"
            >&nbsp;to process this file into a dataset of audio features like
            beat, tempo, energy, amplitude, and frequency. We will start by
            looking at the fine grained sound features and then average these
            out for more general understandings of each sound file.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h4 class="c8" id="h.piuj175sgbw8">
          <span class="c1 c19">What is Sound and How Do We Represent It?</span>
        </h4>
        <p class="c12">
          <span class="c0"
            >Sound vibrates through a medium (like air or water) and travels in
            waves. When we listen to sound in real life, we are processing many
            sound waves added onto each other. That seems similar to a Fourier
            series which is essentially a sum of</span
          ><span>&nbsp;sine</span
          ><span class="c1 c0"
            >&nbsp;waves that can approximate functions like sound signals.
            Therefore, it makes sense to deconstruct these sums of waves using
            Fourier Transforms which gives us the coefficients of different
            frequency waves to match our sound file. These coefficients tell us
            how big or small the amplitude of the sound wave for that frequency
            should be which can help us understand how loud the sound is at
            different parts of the frequency spectrum.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span>However,</span
          ><span class="c1 c0"
            >&nbsp;how large the amplitude of a frequency wave is doesn&rsquo;t
            necessarily correspond to how loud we perceive it to be; for
            example, humans process lower frequencies as louder than higher
            frequencies at the same amplitude. In order to account for this, I
            use the Mel scale, which rebalances the coefficients to better
            represent the perceived loudness of these frequencies. In my
            dataset, I use Fourier transforms combined with the Mel scale to get
            coefficients for each frequency.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c1 c0"
            >I also analyzed features like energy, spectral centroids, and
            chroma at each timestamp of the given audio file.
          </span>
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image2.png" title="" /></span>
        </p>
        <p class="c12">
          <span class="c0"
            >Spectral centroids represent the center of mass of the spectrum (if
            we think of sound as lying on a spectrum and giving weight to
            different parts of this spectrum based on the loudness of different
            frequencies, the center of mass would be our spectral centroid).
            This is very useful in getting an overall picture of how frequencies
            are changing throughout the audio file. </span
          ><span class="c0">Chromagrams</span
          ><span class="c0"
            >&nbsp;help us understand the specific music notes of an audio file;
            for this feature, I select the active notes at each time step by
            detecting which chroma values are above a certain threshold. </span
          ><sup class="c0"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup>
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image5.png" title="" /></span>
        </p>
        <h5 class="c5 c25" id="h.7fm4gxb6evr3">
          <span class="c2">Example of a Chromagram</span>
        </h5>
        <p class="c6"><span class="c1 c0"></span></p>
        <h4 class="c8" id="h.z3jh4htfszum">
          <span class="c1 c19">Data Aggregations</span>
        </h4>
        <p class="c12">
          <span class="c1 c0"
            >Using the fine grained audio features extracted for each scene, I
            computed aggregate values for each feature. For energy and spectral
            centroids I computed the average, and for amplitude I saved the
            maximum amplitude. For the notes determined by the chromatograms, I
            saved a list of all notes played during the audio file. I also used
            the librosa library to compute audio features over the entire audio
            file instead of the small timestamps in the previous features. This
            includes features like tempo and beat.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0">Finally, I used machine learning to make </span
          ><span>predictions</span><span class="c0">&nbsp;</span><span>for</span
          ><span class="c0">&nbsp;the</span><span>&nbsp;</span
          ><span class="c1 c0"
            >emotions in the audio file. This was motivated by my curiosity as
            to how emotions can be conveyed in music and how they might relate
            or contrast to the film plots.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16" id="h.sfo0u1d3wiki"><span>Emotion Detection </span></h3>
        <p class="c12">
          <span
            >The dataset I worked with was from the research study
            &quot;Multi-Label Classification of Music by Emotion&quot; which had
            a group of people label the emotions that they felt after listening
            to each music piece.</span
          ><span class="c0">&nbsp;</span
          ><sup class="c0"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup
          ><span class="c0">&nbsp;The data consist</span
          ><span class="c1 c0"
            >s of the audio features (Mfcc constants, spectral centroids,
            chromagram features, and more) for a sound file and the
            corresponding emotion label. There were around 300 entries in the
            entire dataset which I split 80:20 for my train:test split.</span
          >
        </p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c12">
          <span
            >I used a random forest classification model which yielded a low
            classification error on both training and test sets. However, this
            dataset is relatively small, and I wanted to see how this model
            performed on some of my actual data&ndash;do these emotion labels
            match my perceived emotions for different film scores? </span
          ><span><img alt="" src="./styles/images/image10.png" title="" /></span
          ><span
            >In order to apply this classifier to new data, I created an
            audioFeatureExtraction class and used the </span
          ><span class="c7">librosa</span><span class="c23">&nbsp;</span
          ><span class="c1 c0"
            >python library to compute the same audio features from my .wav
            files as the training dataset used. I then loaded my audio files,
            extracted the audio features, loaded my pretrained model, ran
            predictions on my audio files, and saved the resulting labels in a
            csv file. </span
          ><span
            ><img alt="" src="./styles/images/image16.png" title=""
          /></span>
        </p>
        <p class="c12">
          <span
            >Finally, I decided to aggregate the same colors I had extracted
            from these films with the emotion labels to see if there were
            correlations between certains emotions and colors. The results
            revealed that more warm tones corresponded to the
            &lsquo;happy-pleased&rsquo; label while more cool tones corresponded
            to the &lsquo;sad-lonely&rsquo; label.
          </span>
        </p>
        <p class="c6"><span class="c1 c19"></span></p>
        <hr />
        <p class="c6"><span class="c17"></span></p>
        <h2 class="step" id="h.5i3hf2t1van">
          <span class="c20">Sound Sketches</span>
        </h2>
        <p class="c12">
          <span class="c0"
            >I explored sound through a series of p5.js sketches which
            synchronize with the audio being played</span
          ><span>, translating</span
          ><span class="c0"
            >&nbsp;the audio data into different visuals. These sketches are
            data driven as the colors are pulled from our imageSceneData</span
          ><span>, while </span
          ><span class="c1 c0"
            >our audioSceneData dictates the motion and shapes of our visuals. I
            was inspired by cinematography and how elements like repetition,
            symmetry, color, motion blur, light, framing, and scale can be used
            to set tones and convey plot. In my sketches I wanted to explore how
            these ideas can extend into sound.</span
          >
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image11.png" title="" /></span>
        </p>
        <p class="c12">
          <span class="c0"
            >To start, I was curious about how captions and text can reflect the
            sentiments of dialogue and music. My first dynamic type sketch works
            by transforming the text into lists of points indicating the outer
            shape of each letter. Working with points allowed me to manipulate
            the text and disto</span
          ><span
            >rt its shape. One concept that I use throughout my sketches is
            mapping the amplitudes of frequencies determined by the FFTs (which
            I then rescale and normalize using the Mel scale as described
            previously) to different aspects of a sketch. In this exploration of
            type and sound, I divide the frequency spectrum into bins evenly for
            each letter, and then map the av</span
          ><span class="c0"
            >erage amplitude of that letter bin to a feature. In this example, </span
          ><span>I first</span><span class="c0">&nbsp;map</span
          ><span>&nbsp;the</span
          ><span class="c0"
            >&nbsp;amplitude to the vertical position and scale of </span
          ><span>the text</span
          ><span class="c0"
            >. This means the louder that range of the spectrum is, the bigger
            and higher up the letter will move. </span
          ><span><img alt="" src="./styles/images/image13.png" title="" /></span
          ><span>Next, I mapped the frequency constants to the</span
          ><span class="c0">&nbsp;</span><span>size</span
          ><span class="c0">&nbsp;</span><span>of</span
          ><span class="c0"
            >&nbsp;the noise movement which means that a louder sound will make
            the letters warp at a faster rate. I created this effect by adding
            noise to each of the points on the letter and animating it by using
            p5&rsquo;s frameCount variable. </span
          ><span>Combined together, </span
          ><span class="c0">these amplitude mappings </span
          ><span>drive the text to vibrate, grow, and move.</span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c0"
            >This second sketch can be used on both text and image inputs. It
            works by first drawing the text or image onto the blank p5 canvas,
            then reading each pixel of the canvas, and saving its coordinates
            into a data structure if the color of that pixel is </span
          ><span class="c0 c7">close enough</span
          ><sup class="c0 c7"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup
          ><span class="c0 c7">&nbsp;</span
          ><span class="c0">to some inputted color.</span
          ><span><img alt="" src="./styles/images/image9.png" title="" /></span
          ><span class="c0"
            >&nbsp;I then bin 360 degrees into n bins, and stretch lines
            stemming from each pixel towards the angle of each bin based on the
            frequency amplitudes. </span
          ><span class="c1 c0"
            >I used this similar idea for another font, &ldquo;strings&rdquo;,
            which emulates the plucking of string instruments by mapping the
            audio values to the curve pull of each string. Instead of processing
            an image, I get the lines for each font by simplifying the text
            points by their x coordinates (two points are the same if they are
            within some x_threshold distance of each other). This idea is
            illustrated below.</span
          >
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image15.png" title="" /></span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span
            >My final font sketches worked with flow fields. I created a </span
          ><span class="c7">Particle </span
          ><span class="c1 c0"
            >class for each point on the text, initializing it with a velocity
            (speed + direction) and position. Using a similar frequency binning
            process, I assign each particle to an audio value, and as the audio
            value increases or decreases, so does the speed of the particle and
            the magnitude of the noise applied to its position. The direction of
            the particle is modified at each frame of the animation to point
            towards the top of the canvas using basic trigonometric functions;
            once the particle reaches the top of the canvas, it redirects the
            particle to its starting position. &nbsp;</span
          >
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image6.png" title="" /></span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <hr />
        <p class="c6"><span class="c1 c0"></span></p>
        <h2 class="step" id="h.q1oj0w7uywpb"><span>Design </span></h2>
        <p class="c12">
          <span class="c1 c0"
            >In order to keep the focus of this project on the art of film,
            sound, and image, I decided to keep the external interface minimal.
            I chose primary colors of black, and white for all of the functional
            elements, while using the colors that I extracted from the scenes to
            illustrate all of the film specific data visualizations in order to
            make the experience more immersive. To balance making the interface
            easy to navigate while also encouraging exploration, I created a
            generous interface of all the scenes of a film which users can click
            on to see more. This is in the opening page which gives users a
            preview of many of the features in the page including the dynamic
            fonts, color extractions, and range of movie scenes represented. As
            the user hovers over different scene previews, the audio plays which
            then drives the text to respond.</span
          ><span><img alt="" src="./styles/images/image14.png" title="" /></span
          ><span><img alt="" src="./styles/images/image4.png" title="" /></span>
        </p>
        <h3 class="c16" id="h.2oa3nl5nso9i">
          <span class="c1 c11">Video Player Interface</span>
        </h3>
        <p class="c12">
          <span class="c1 c0"
            >This interface serves three main functions. It allows users to
            watch and control the video, experience and modify the typography,
            and see the live instruments and colors extracted from the audio. If
            the scene has caption data, the typography should also sync with the
            live captions of the video.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image18.png" title="" /></span>
        </p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c19"></span></p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span class="c1 c0"
            >I wanted this interface to feel multi sensory; this meant capturing
            the sound, colors, and tones of each film. In order to do that, I
            wanted to make the CSS dependent on the image data that I extracted
            from each video. To start, I created a dark/light mode option which
            toggles the background and interface colors from light to dark; this
            is intended to match the darkness level of the different films.
            Secondly, I created a color mode called &ldquo;sync with
            image&rdquo; which sets the colors of the interface to change based
            on the colors of the image. Instead of changing the color at every
            second, this function is only called when a &ldquo;beat&rdquo; is
            detected in the audio; this is another audio feature which I
            extracted using a tempogram.</span
          >
        </p>
        <p class="c12">
          <span><img alt="" src="./styles/images/image17.png" title="" /></span>
        </p>
        <p class="c12">
          <span
            >I also wanted to expose as many parameters to my fonts as possible
            to encourage exploration as each film might have a different font,
            audio range, or color mode that works best. The bottom bar contains
            the caption value which will sync with caption data if available,
            but is also a modifiable input. The &ldquo;text type&rdquo; input
            allows users to select between 5 dynamic font options from my
            library including </span
          ><span class="c23 c7">flow</span><span class="c7">,</span
          ><span class="c23 c7">&nbsp;blur</span><span class="c7">,</span
          ><span class="c23 c7">&nbsp;swirl</span><span class="c7">,</span
          ><span class="c23 c7">&nbsp;strings</span
          ><span class="c7">, and </span><span class="c23 c7">wiggly. </span
          ><span>Finally, the users can select the input to the fonts: </span
          ><span class="c23 c7">mfcc</span><span>&nbsp;or </span
          ><span class="c23 c7">chroma</span
          ><span
            >. These constants were described in the audio processing section;
            essentially the </span
          ><span class="c23 c7">chroma</span
          ><span
            >&nbsp;highlights the different notes in the signal while the </span
          ><span class="c7 c23">mfcc</span
          ><span>&nbsp;represents the loudness of each frequency that we </span
          ><span class="c7">perceive. </span
          ><span class="c1 c0"
            >Changing this input will change which features will drive the
            font&rsquo;s movement.</span
          >
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h3 class="c16" id="h.li18qrvm5gda">
          <span class="c1 c11">Movie Dashboard</span>
        </h3>
        <p class="c12">
          <span class="c1 c0"
            >This view allows users to navigate the different scenes of the
            larger film while also viewing sound trends of the film through
            time. The line plot on the top syncs its x-axis with the timestamps
            of the scenes laid out on the bottom and depicts the spectral
            centroid. This feature is sampled hundreds of times per scene, so in
            order to simplify the trends, I plotted the average of the spectral
            centroid within some chunk interval. On the selected scene, it plots
            every sample, on the previous and next scene, it plots the average
            for every 10 samples, and for the remaining scenes, it averages over
            every 50 samples. With this dynamic plot, I was interested in
            playing with scales and decided that changing the precision based on
            the selected scene offered both a detailed and general picture of
            the data. This is because without the aggregations, the line plot
            looks very chaotic and it&rsquo;s difficult to notice trends, but
            with the aggregations, we lose our visibility of outliers. </span
          ><span><img alt="" src="./styles/images/image8.png" title="" /></span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c12">
          <span
            >The second plot depicts the chromagram data. Since a chromagram
            helps pick out notes on a piano (regardless of the octave), I
            decided to use a piano to depict this. Each set of piano keys
            represents the notes found in the corresponding scene, and the notes
            are filled in if they are played. I was interested to see if this
            helps us notice dominant keys or notable key changes in the film.
            Furthermore, I added a feature that plays the selected keys in a
            scene when you click it in order to bridge the visual understanding
            of the piano with our intuitive understanding of notes.</span
          >
        </p>
        <h2 class="step" id="h.lqbae7wmunfb">
          <span class="c1 c13">Conclusion</span>
        </h2>
        <h4 class="c26 c25" id="h.pb453qcm6hxw">
          <span class="c1 c14">Summary</span>
        </h4>
        <p class="c12">
          <span class="c1 c0"
            >In this thesis, I created an audio driven interface that allows
            users to watch a film with real time, audio reactive captions,
            instrument illustrations, and beat sync-ed colors.</span
          >
        </p>
        <p class="c12">
          <span class="c1 c0"
            >This immersive interface ultimately works towards my goal of
            visualizing the sound of movies through type, color, and
            illustration. Namely, the dynamic font library works to capture the
            intensity, emotion, and pacing of film dialogue that we miss in
            traditional captioning. Furthermore, the interface colors are driven
            by the video&#39;s stills, creating an interface that feels like an
            extension of the film itself. Through this project, I also created
            my own data generation pipeline pipeline that takes a video,
            sections it into scenes, extracts the main colors at each second,
            and computes the audio data using spectral analysis and emotion
            classification. Therefore, this project can be extended to more
            films and used as a tool for generating typography from music.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h4 class="c25 c26" id="h.6sipkal8mn0b">
          <span class="c1 c14">Limitations and Future Directions</span>
        </h4>
        <p class="c12">
          <span class="c1 c0"
            >In this project, I faced several challenges. To start, the
            readability of different fonts can vary depending on the audio
            inputs and selected fonts. I partially addressed this by listing the
            caption in plane text while also exposing the different font
            parameters and font types to allow users to tweak the fonts to
            improve legibility. Secondly, larger videos like entire films
            &nbsp;can reduce the performance and increase the loading times of
            the site. Consequently, I mainly used single scenes, but am
            therefore lacking some insights on overall audio trends across a
            film in its entirety. I also struggled with getting the captions for
            different films in a consistent file format which I hope to remedy
            by extending the compatible caption formats. I am also interested in
            using LLMs to generate my own caption files from the audio
            files.</span
          >
        </p>
        <p class="c6"><span class="c0 c1"></span></p>
        <p class="c12">
          <span
            >One of my main goals going forward, is to add functionality for
            users to upload their own videos which the backend will then process
            and load onto the site. </span
          ><span class="c1 c0"
            >I also hope to integrate the emotion labeling and visualize more
            trends of the music across the entire plot. In general I want to
            keep exploring more of the larger scheme trends in the audio data
            over films, composers, and genres. For example, in my emotion
            detection, it was interesting to see how the colors correlated to
            different emotions. I could extend my current loading screen to also
            show the scenes clustered by emotions, color, and genre.
          </span>
        </p>
        <p class="c6"><span class="c1 c0"></span></p>
        <h4 class="c26 c25" id="h.n6vulxb9m3wx">
          <span class="c1 c14">Reflection</span>
        </h4>
        <p class="c12">
          <span class="c1 c0"
            >Film is a multisensory and interdisciplinary art form that can be
            enhanced through data visualization to create a more accessible and
            immersive experience. I hope this project can be a space for
            exploring and enjoying the visual beauty of sound, and the
            intersection of music, narrative, and design.</span
          >
        </p>

        <div style="height: 200px"></div>
        <h2 class="step" id="h.pet4x97du9ly">
          <span class="c1 c13">Bibliography</span>
        </h2>
        <p class="c12">
          <span class="c0"
            >Calgary Philharmonic Orchestra. &ldquo;A Brief History of Film
            Music.&rdquo; </span
          ><span class="c0 c7">Calgary Philharmonic Orchestra</span
          ><span class="c0">, accessed April 1, 2025. </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://calgaryphil.com/blog-a-brief-history-of-film-music/&amp;sa=D&amp;source=editors&amp;ust=1748620336100837&amp;usg=AOvVaw3psbhLO1MLysYK-8AtWDzy"
              >https://calgaryphil.com/blog-a-brief-history-of-film-music/</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Blaszke, Maciej, and Bo&#380;ena Kostek. &quot;Musical Instrument
            Identification Using Deep Learning Approach.&quot; </span
          ><span class="c0 c7">Sensors</span
          ><span class="c1 c0"
            >(Gda&#324;sk University of Technology). Accessed March 31,
            2025.</span
          >
        </p>
        <p class="c21">
          <span class="c0"
            >Gernsbacher, Morton Ann. &ldquo;Video Captions Benefit
            Everyone.&rdquo; </span
          ><span class="c0 c7">The Permanente Journal</span
          ><span class="c0">&nbsp;21 (2017): 16-230. </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://pmc.ncbi.nlm.nih.gov/articles/PMC5214590/&amp;sa=D&amp;source=editors&amp;ust=1748620336101711&amp;usg=AOvVaw3GMnD-6Hvc1AUSf6XmtMnV"
              >https://pmc.ncbi.nlm.nih.gov/articles/PMC5214590/</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Green, Jessica. &quot;Understanding the Score: Film Music
            Communicating to and Influencing the Audience.&quot; </span
          ><span class="c0 c7">The Journal of Aesthetic Education</span
          ><span class="c0">&nbsp;44, no. 4 (December 2010): 81&ndash;94. </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://doi.org/10.1353/jae.2010.0009&amp;sa=D&amp;source=editors&amp;ust=1748620336102209&amp;usg=AOvVaw1L2bWywXsPmumfdvDNva9K"
              >https://doi.org/10.1353/jae.2010.0009</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Lu, Yun-Jhen, I-Chun Kuo, and Ming-Chou Ho. &quot;The Effects of
            Emotional Films and Subtitle Types on Eye Movement Patterns.&quot; </span
          ><span class="c0 c7">Acta Psychologica</span
          ><span class="c1 c0">&nbsp;230 (2022).</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Ikhsan, Sandi Putra. &quot;Analysis of the Structural Complexity of
            &#39;Time&#39; in Hans Zimmer&rsquo;s Score for </span
          ><span class="c0 c7">Inception</span><span class="c0">.&quot; </span
          ><span class="c0 c7">Interlude</span
          ><span class="c0">&nbsp;3, no. 1 (November 2023). </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://doi.org/10.17509/interlude.v3i1.70083&amp;sa=D&amp;source=editors&amp;ust=1748620336103082&amp;usg=AOvVaw1B2MYsWNZHBQh6lPhlgE02"
              >https://doi.org/10.17509/interlude.v3i1.70083</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Trohidis, Konstantinos, Grigorios Tsoumakas, George Kalliris, and
            Ioannis Vlahavas. &quot;Multi-Label Classification of Music by
            Emotion.&quot; </span
          ><span class="c0 c7"
            >EURASIP Journal on Audio, Speech, and Music Processing</span
          ><span class="c0">&nbsp;2011, no. 4 (2011). </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=http://asmp.eurasipjournals.com/content/2011/1/4&amp;sa=D&amp;source=editors&amp;ust=1748620336103737&amp;usg=AOvVaw0aDtjPDKaXj-xe4EWxYW9j"
              >http://asmp.eurasipjournals.com/content/2011/1/4</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Sable, Anuj. &quot;Introduction to Audio Analysis and
            Processing.&quot; </span
          ><span class="c0 c7">Paperspace Blog</span
          ><span class="c0"
            >. Machine Learning, 4 years ago. Accessed March 31, 2025. </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://blog.paperspace.com/introduction-to-audio-analysis-and-synthesis/&amp;sa=D&amp;source=editors&amp;ust=1748620336104296&amp;usg=AOvVaw2ddKJhi95jONNUm7UHaD3L"
              >https://blog.paperspace.com/introduction-to-audio-analysis-and-synthesis/</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21">
          <span class="c0"
            >Sparsh, I.M. &quot;MusicNet Dataset.&quot; Kaggle. Accessed March
            31, 2025. </span
          ><span class="c4 c0"
            ><a
              class="c9"
              href="https://www.google.com/url?q=https://www.kaggle.com/datasets/imsparsh/musicnet-dataset/data&amp;sa=D&amp;source=editors&amp;ust=1748620336104664&amp;usg=AOvVaw2stxNnTVEH4xVB1nvZXQKk"
              >https://www.kaggle.com/datasets/imsparsh/musicnet-dataset/data</a
            ></span
          ><span class="c1 c0">.</span>
        </p>
        <p class="c21 c34"><span class="c1 c0"></span></p>
        <p class="c6"><span class="c1 c0"></span></p>
        <p class="c6"><span class="c1 c0"></span></p>
        <hr class="c27" />
        <div>
          <p class="c5">
            <a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c3">&nbsp;</span
            ><span
              >Green, Jessica. &quot;Understanding the Score: Film Music
              Communicating to and Influencing the Audience.&quot; </span
            ><span class="c7">The Journal of Aesthetic Education</span
            ><span>&nbsp;44, no. 4 (December 2010): 81&ndash;94. </span
            ><span class="c4"
              ><a
                class="c9"
                href="https://www.google.com/url?q=https://doi.org/10.1353/jae.2010.0009&amp;sa=D&amp;source=editors&amp;ust=1748620336108402&amp;usg=AOvVaw3_bumo9EAjV5azSz2wQ6BM"
                >https://doi.org/10.1353/jae.2010.0009</a
              ></span
            ><span>.</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref2" id="ftnt2">[2]</a
            ><span
              >&nbsp;Robert Stam, Robert Burgoyne, and Sandy Flitterman-Lewis, </span
            ><span
              >New Vocabularies in Film Semiotics: Structuralism,
              Post-Structuralism and Beyond</span
            ><span>&nbsp;(London: Routledge, 1992)</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref3" id="ftnt3">[3]</a
            ><span class="c3"
              >&nbsp;Museum of Modern Art. &ldquo;Experimentation with
              Sound.&rdquo; </span
            ><span class="c3 c7">MoMA</span
            ><span class="c3">, accessed April 1, 2025. </span
            ><span class="c4 c3"
              ><a
                class="c9"
                href="https://www.google.com/url?q=https://www.moma.org/collection/terms/film/experimentation-with-sound%23:~:text%3DOn%2520October%25206%252C%25201927%252C%2520Warner,was%2520a%2520revelation%2520for%2520audiences&amp;sa=D&amp;source=editors&amp;ust=1748620336106796&amp;usg=AOvVaw2DDzDCTyg9hytq633ydm8Q"
                >https://www.moma.org/collection/terms/film/experimentation-with-sound</a
              ></span
            ><span class="c1 c10 c3">.</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref4" id="ftnt4">[4]</a
            ><span class="c3"
              >&nbsp;Calgary Philharmonic Orchestra. &ldquo;A Brief History of
              Film Music.&rdquo; </span
            ><span class="c3 c7">Calgary Philharmonic Orchestra</span
            ><span class="c3">, accessed April 1, 2025. </span
            ><span class="c3 c4"
              ><a
                class="c9"
                href="https://www.google.com/url?q=https://calgaryphil.com/blog-a-brief-history-of-film-music/&amp;sa=D&amp;source=editors&amp;ust=1748620336107386&amp;usg=AOvVaw2HcQjqOfxBOvjI5uHBd24L"
                >https://calgaryphil.com/blog-a-brief-history-of-film-music/</a
              ></span
            ><span class="c1 c10 c3">.</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref5" id="ftnt5">[5]</a
            ><span class="c3"
              >&nbsp;Gernsbacher, Morton Ann. &ldquo;Video Captions Benefit
              Everyone.&rdquo; </span
            ><span class="c3 c7">The Permanente Journal</span
            ><span class="c3">&nbsp;21 (2017): 16-230. </span
            ><span class="c4 c3"
              ><a
                class="c9"
                href="https://www.google.com/url?q=https://pmc.ncbi.nlm.nih.gov/articles/PMC5214590/&amp;sa=D&amp;source=editors&amp;ust=1748620336107844&amp;usg=AOvVaw3ozVBZQZV5yiwtfu1UrNbl"
                >https://pmc.ncbi.nlm.nih.gov/articles/PMC5214590/</a
              ></span
            ><span class="c1 c10 c3">.</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref6" id="ftnt6">[6]</a
            ><span class="c1 c10 c3">&nbsp;</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c3">&nbsp;</span
            ><span
              >Trohidis, Konstantinos, Grigorios Tsoumakas, George Kalliris, and
              Ioannis Vlahavas. &quot;Multi-Label Classification of Music by
              Emotion.&quot; </span
            ><span class="c7"
              >EURASIP Journal on Audio, Speech, and Music Processing</span
            ><span>&nbsp;2011, no. 4 (2011). </span
            ><span class="c4"
              ><a
                class="c9"
                href="https://www.google.com/url?q=http://asmp.eurasipjournals.com/content/2011/1/4&amp;sa=D&amp;source=editors&amp;ust=1748620336106203&amp;usg=AOvVaw0zVEnL6B5ZS9d89H9Ojxrh"
                >http://asmp.eurasipjournals.com/content/2011/1/4</a
              ></span
            ><span>.</span>
          </p>
        </div>
        <div>
          <p class="c5">
            <a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c3">&nbsp;</span
            ><span class="c3 c7">Close enough</span
            ><span class="c1 c10 c3"
              >&nbsp;- a color is close enough to another color if the euclidean
              distance between them is less than some constant threshold. The
              euclidean distance reverse to the distance between the colors in
              R3 space where the R,G,B components of each color determines its
              coordinates</span
            >
          </p>
        </div>
      </div>
    </div>
  </body>
</html>
